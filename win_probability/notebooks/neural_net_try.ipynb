{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('../data/sixth_df_input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['Rank'] = data_df['Rank'].astype('int')\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_map_sex = {'men': 1, 'women': 0}\n",
    "data_df['sex'] = data_df['sex'].map(dict_map_sex)\n",
    "\n",
    "dict_map_format = {'sprint' : 0, 'pursuit': 1, 'individual': 2, 'mass_start': 3, 'sprint_2': 0, 'pursuit_2': 1}\n",
    "data_df['format'] = data_df['format'].map(dict_map_format)\n",
    "\n",
    "def rank_transform(x):\n",
    "    if x == 1:\n",
    "        return 0\n",
    "    elif x == 2 or x == 3:\n",
    "        return 1\n",
    "    elif x > 3:\n",
    "        return 2\n",
    "\n",
    "data_df['Rank'] = data_df['Rank'].apply(rank_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = data_df[['season', 'Rank', 'distance', 'Behind', 'format', 'position', 'km_to_go', 'standing_remaining', 'prone_remaining', 'probability_standing',\\\n",
    "    'probability_prone', 'back_from_median', 'gradient_back', 'all_mistakes_prone', 'all_mistakes_standing', 'gradient_standing', 'gradient_prone',\\\n",
    "        'gradient_pos', 'behind_above', 'behind_below', 'standing_above', 'standing_below', 'prone_above', 'prone_below']]\n",
    "\n",
    "final_df_train = final_df[final_df['season'] != '2021-2022']\n",
    "final_df_test = final_df[final_df['season'] == '2021-2022']\n",
    "\n",
    "final_df_train.drop(['season'], axis = 1, inplace = True)\n",
    "final_df_test.drop(['season'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    " \n",
    "  def __init__(self, df):\n",
    "    \n",
    "    x=df.iloc[:,1:].values\n",
    "    y=df.iloc[:,0].values\n",
    " \n",
    "    self.x_train=torch.tensor(x, dtype=torch.float32)\n",
    "    self.y_train=torch.tensor(y, dtype=torch.float32)\n",
    " \n",
    "  def __len__(self):\n",
    "    return len(self.y_train)\n",
    "   \n",
    "  def __getitem__(self,idx):\n",
    "    return self.x_train[idx], self.y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = MyDataset(final_df_train)\n",
    "test_df = MyDataset(final_df_test)\n",
    "print(train_df[0], '\\n', test_df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=DataLoader(train_df,batch_size=100,shuffle=True)\n",
    "test_loader=DataLoader(test_df,batch_size=len(test_df),shuffle=False)\n",
    "\n",
    "for i, (data, labels) in enumerate(train_loader):\n",
    "  print(data.shape, labels.shape)\n",
    "  print(data, labels)\n",
    "  break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = sklearn.utils.class_weight.compute_class_weight('balanced', classes = np.unique(final_df_train['Rank']), y = final_df_train['Rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_torch = torch.tensor(weights, dtype=torch.float32)\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss(weight = weights_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = ['Adam', 'SGD', 'RMSprop']\n",
    "architectures = [[22, 50, 50, 3], [22, 100, 3], [22, 10, 10, 10, 10, 10, 3]]\n",
    "normalized_input = [True, False]\n",
    "batch_sizes = [100, 200, 500]\n",
    "weight_decay = [1e-3, 1e-4, 1e-5]\n",
    "activation_functions = [nn.ReLU(), nn.Sigmoid()]\n",
    "epochs = 30\n",
    "batches = [True, False]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, architecture, activation, batch):\n",
    "\n",
    "\n",
    "        self.ord_dict = OrderedDict([])\n",
    "        for i, layer in enumerate(architecture):\n",
    "            if i > 0:\n",
    "                \n",
    "                self.ord_dict.update({f'layer_{i}': \\\n",
    "                    nn.Linear(architecture[i-1], architecture[i])})\n",
    "\n",
    "                \n",
    "\n",
    "                if i < len(architecture) - 1:\n",
    "                    if batch:\n",
    "                        self.ord_dict.update({f'Batch_{i}': nn.BatchNorm1d(architecture[i])})\n",
    "                    self.ord_dict.update({f'activation_{i}': activation})\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        # feed forward network\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            # First convolutional layer\n",
    "            self.ord_dict,\n",
    "        )\n",
    "        \n",
    "       # forward part of the network\n",
    "    def forward(self, x):\n",
    "        x = self.feed_forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = np.zeros(len(optimizers) * len(architectures) * len(normalized_input) * len(batch_sizes) * len(weight_decay) * len(activation_functions) *\\\n",
    "     len(batches), dtype = object)\n",
    "i = 0\n",
    "for optimizer in optimizers:\n",
    "    for architecture in architectures:\n",
    "        for normalized in normalized_input:\n",
    "            for batch_size in batch_sizes:\n",
    "                for weight in weight_decay:\n",
    "                    for activation in activation_functions:\n",
    "                        for batch in batches:\n",
    "                        \n",
    "                            all_models[i] = {'optimizer': optimizer, 'architecture': architecture, 'normalized': normalized,\\\n",
    "                                'batch_size': batch_size, 'weight_decay': weight, 'activation': activation, 'batch': batch}\n",
    "                            i += 1\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = MyDataset(final_df_train)\n",
    "columns_list = list(final_df_train.columns)\n",
    "final_df_train_2 = final_df_train.copy()\n",
    "final_df_train_2[columns_list[1:]] = final_df_train_2[columns_list[1:]].apply(lambda x: (x - x.mean()) / (x.std()))\n",
    "\n",
    "final_df_test_2 = final_df_test.copy()\n",
    "final_df_test_2[columns_list[1:]] = final_df_test_2[columns_list[1:]].apply(lambda x: (x - x.mean()) / (x.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2 = MyDataset(final_df_train_2)\n",
    "test_df_2 = MyDataset(final_df_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "kf = KFold(n_splits = n_folds, shuffle = True, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAINING LOOP\n",
    "num_epochs = 50\n",
    "fold_losses = []\n",
    "train_loss_log = {}\n",
    "val_loss_log = {}\n",
    "\n",
    "l1_loss = nn.L1Loss()\n",
    "\n",
    "for activation_ in list(activations.keys()):   \n",
    "    for reg in regularizations:\n",
    "        for l1 in l1_weights:\n",
    "            for l2 in l2_weights:\n",
    "                for lr in learning_rates:\n",
    "                    for optimizer_ in optimizers:\n",
    "                        for architecture_ in list(architecures.keys()):\n",
    "                            print((activation_, reg, l1, l2, lr, optimizer_, architecture_))\n",
    "                            train_loss_log[(activation_, reg, l1, l2, lr, optimizer_, architecture_)] = {}\n",
    "                            val_loss_log[(activation_, reg, l1, l2, lr, optimizer_, architecture_)] = {}\n",
    "                            fold_index = 0\n",
    "\n",
    "                            #print((loss_, activation_, reg, l1, l2, lr, optimizer))\n",
    "                            for train_idx, val_idx in kf.split(np.array(train_df)):\n",
    "                            \n",
    "                                net = Net(architecures[architecture_], activations[activation_])\n",
    "                                if optimizer_ == 'Adam':\n",
    "                                    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "                                elif optimizer_ == 'RMSprop':\n",
    "                                    optimizer = optim.RMSprop(net.parameters(), lr=lr)\n",
    "\n",
    "                                elif optimizer_ == 'SGD_with_momentum':\n",
    "                                    optimizer = optim.SGD(net.parameters(), lr=lr, momentum = 0.9)\n",
    "                                fold_index += 1\n",
    "                                \n",
    "                                train_loss_log[(activation_, reg, l1, l2, lr, optimizer_, architecture_)][f'fold_{fold_index}'] = []\n",
    "                                train_sample = RegressionDataset(train_df.iloc[train_idx], transform= \\\n",
    "                                lambda x: (torch.Tensor([x[0]]).float(),torch.Tensor([x[1]]).float()))\n",
    "\n",
    "                                val_loss_log[(activation_, reg, l1, l2, lr, optimizer_, architecture_)][f'fold_{fold_index}'] = []\n",
    "                                validation_sample = RegressionDataset(train_df.iloc[val_idx], transform= \\\n",
    "                                lambda x: (torch.Tensor([x[0]]).float(),torch.Tensor([x[1]]).float()))\n",
    "\n",
    "                                train_data = DataLoader(train_sample, batch_size=4, shuffle=False)\n",
    "                                validation_data = DataLoader(validation_sample, batch_size=len(validation_sample), shuffle=False)\n",
    "                                \n",
    "                                for epoch_num in range(num_epochs):\n",
    "                                    \n",
    "\n",
    "                                    ### TRAIN\n",
    "                                    train_loss= []\n",
    "                                    net.train() # Training mode (e.g. enable dropout, batchnorm updates,...)\n",
    "                                    for sample_batched in train_data:\n",
    "\n",
    "                                        \n",
    "                                        x_batch = sample_batched[0]\n",
    "                                        label_batch = sample_batched[1]\n",
    "                                        \n",
    "                                        # Forward pass\n",
    "                                        out = net(x_batch)\n",
    "\n",
    "                                        # Compute loss\n",
    "                                        loss_fn = l1_loss\n",
    "                                        \n",
    "                                        loss = loss_fn(out, label_batch)\n",
    "                                        \n",
    "                                        l1_penalty = l1 * sum([p.abs().sum() for name, p in net.named_parameters()\\\n",
    "                                            if 'bias' not in name])\n",
    "                                        l2_penalty = l2 * sum([(p**2).sum() for name, p in net.named_parameters()\\\n",
    "                                            if 'bias' not in name])\n",
    "                                        \n",
    "                                        if reg == 'L1':\n",
    "                                            loss += l1_penalty \n",
    "                                        elif reg == 'L2':\n",
    "                                            loss += + l2_penalty \n",
    "                                        elif reg == 'L2':\n",
    "                                            loss += + l2_penalty + l1_penalty\n",
    "                                        # Backpropagation\n",
    "                                        net.zero_grad()\n",
    "                                        loss.backward()\n",
    "\n",
    "                                        # Update the weights\n",
    "                                        optimizer.step()\n",
    "\n",
    "                                        # Save train loss for this batch\n",
    "                                        loss_batch = loss.detach().cpu().numpy() # we detach it from the computational graph\n",
    "                                        train_loss.append(loss_batch)\n",
    "\n",
    "                                    # Save average train loss\n",
    "                                    train_loss = np.mean(train_loss)\n",
    "                                    \n",
    "                                    \n",
    "                                    train_loss_log[( activation_, reg, l1, l2, lr, optimizer_, architecture_)][f'fold_{fold_index}']\\\n",
    "                                        .append(train_loss)\n",
    "\n",
    "                                    ### VALIDATION\n",
    "                                    \n",
    "                                    val_loss= []\n",
    "                                    net.eval() # Evaluation mode (e.g. disable dropout, batchnorm,...)\n",
    "                                    with torch.no_grad(): # Disable gradient tracking, we don't want to update the weights\n",
    "                                        for sample_batched in validation_data:\n",
    "                                            \n",
    "\n",
    "                                            x_batch = sample_batched[0]\n",
    "                                            \n",
    "                                            label_batch = sample_batched[1]\n",
    "                                            \n",
    "                                            # Forward pass\n",
    "                                            \n",
    "                                            out = net(x_batch)\n",
    "                                \n",
    "                                            # Compute loss\n",
    "                                            loss_fn = l1_loss\n",
    "                                            \n",
    "                                            loss = loss_fn(out, label_batch)\n",
    "\n",
    "                                            if reg == 'L1':\n",
    "                                                loss += + l1_penalty \n",
    "                                            elif reg == 'L2':\n",
    "                                                loss += + l2_penalty \n",
    "                                            elif reg == 'L2':\n",
    "                                                loss += + l2_penalty + l1_penalty\n",
    "\n",
    "                                            # Save val loss for this batch\n",
    "                                            loss_batch = loss.detach().numpy()\n",
    "                                            val_loss.append(loss_batch)\n",
    "\n",
    "                                        # Save average validation loss\n",
    "                                        val_loss = np.mean(val_loss)\n",
    "                                        #print(f\"AVERAGE VAL LOSS: {np.mean(val_loss)}\")\n",
    "                                        val_loss_log[(activation_, reg, l1, l2, lr, optimizer_, architecture_)][f'fold_{fold_index}']\\\n",
    "                                            .append(val_loss)\n",
    "\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
